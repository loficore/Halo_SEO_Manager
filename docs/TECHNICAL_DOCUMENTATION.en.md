# Technical Documentation

## 1. Project Overview

`SEO Manager` is a Node.js application designed to automate the SEO (Search Engine Optimization) metadata generation, validation, and publishing for blog posts. It integrates with the Halo CMS API and utilizes a Large Language Model (LLM) to intelligently generate optimized `metaTitle`, `metaDescription`, and `keywords` for articles, ensuring that this metadata adheres to SEO best practices. The core value of the project is to enhance content discoverability, reduce the burden on content creators to manually optimize SEO, and thereby improve the website's search engine rankings and traffic.

## 2. Project Architecture

The project follows a modular design, with the following main modules and their responsibilities:

*   **`src/index.ts`** (Main Entry Point):
    *   The starting point of the project, responsible for initializing all core components, such as the database, Halo client, SEO optimizer, validator, and publisher.
    *   Coordinates the entire workflow, including fetching articles from Halo, saving articles to the database, starting the scheduler for SEO optimization, and performing a graceful shutdown when the program terminates.
*   **`src/scheduler.ts`** (Scheduler):
    *   Periodically (defaults to every hour) scans the database for articles that need SEO optimization.
    *   Maintains a task queue (`taskQueue`) and adds eligible articles to the queue.
    *   Processes articles in the queue at a fixed interval (every 10 seconds), triggering the SEO optimization, validation, and publishing process.
    *   Implements a graceful start and stop mechanism to ensure that the current task is completed when the program is closed.
*   **`src/haloClient.ts`** (Halo API Interaction):
    *   Encapsulates all interaction logic with the Halo CMS API.
    *   Provides functions for getting the article list, getting the content of a specific article, extracting standardized article information (`ArticleData`) from the original Halo data, and updating the article's SEO metadata.
    *   Handles API request authentication (using an API token) and error logging.
    *   When extracting article data, it filters out deleted or unpublished articles and generates a SHA256 hash of the article content to detect content changes.
*   **`src/seoOptimizer.ts`** (LLM Metadata Generation):
    *   Responsible for interacting with OpenAI (or other LLM services compatible with the OpenAI API) to generate SEO metadata for articles using a large language model.
    *   Generates `metaTitle`, `metaDescription`, `keywords`, and `slug` based on the article content.
    *   Includes a built-in retry mechanism (exponential backoff strategy) to handle LLM API call failures.
    *   Forces the LLM to output metadata in JSON format and has strict requirements for the length of the `metaDescription` (80-160 characters).
*   **`src/seoValidator.ts`** (SEO Metadata Validation):
    *   Performs strict format and length validation on the SEO metadata generated by the LLM.
    *   Validation rules include:
        *   `metaTitle`: Not empty, length not exceeding 60 characters.
        *   `metaDescription`: Not empty, length strictly between 80 and 160 characters.
        *   `keywords`: Not an empty array, containing 2 to 5 keywords, each keyword not empty.
        *   `slug`: Not empty, conforming to the format of short English words plus hyphens (e.g., `my-article-slug`).
*   **`src/seoPublisher.ts`** (SEO Metadata Publishing):
    *   Responsible for publishing the optimized and validated SEO metadata back to the Halo CMS.
    *   Implemented by calling the `updatePostSeoMeta` method of the `HaloClient`.
    *   Records success or failure logs during the publishing process.
*   **`src/database.ts`** (Database Management):
    *   Uses SQLite as the data store, responsible for database initialization and table creation (`articles`, `seo_runs`, `settings`).
    *   Provides CRUD operations for articles, such as saving, getting, and updating.
    *   Manages SEO run records, including the status of each optimization, error messages, number of LLM calls, token usage, time taken, number of attempts, and model version.
    *   Provides the logic for getting the articles that need to be optimized, filtering articles based on the last optimization time, content changes, and forced re-optimization settings.
    *   Manages application settings, such as minimum/maximum content length, minimum optimization interval days, and the forced re-optimization flag.
*   **`src/logger.ts`** (Logging System):
    *   Implements a unified, modular logging system based on Winston.
    *   Supports multiple log levels (`fatal`, `error`, `warn`, `info`, `debug`, `trace`).
    *   Log files are rotated by module and date for easy management and traceability.
    *   Console output supports color differentiation to improve readability.
    *   Provides a `log` function for general logging and `logLLMSeoGeneration` specifically for logging the SEO metadata generated by the LLM for separate analysis.
    *   For a detailed logging strategy summary, see the "Logging Strategy" section.
*   **`src/types/seo.ts`** (Type Definitions):
    *   Defines the `SeoMeta` interface, which standardizes the structure of the SEO metadata (`metaTitle`, `metaDescription`, `keywords`, `slug`).

## 3. Implemented Features

### Halo API Interaction

*   **Get Article List**: Gets multi-page article data from the Halo CMS through the `HaloClient.getAllPosts` method, supporting pagination and delays to avoid API rate limiting.
*   **Get Article Content**: The `HaloClient.getPostContent` method can get the detailed content of an article based on the article ID and try to extract the content from different paths to ensure compatibility.
*   **Update SEO Metadata**: The `HaloClient.updatePostSeoMeta` method is responsible for updating the optimized SEO metadata (`metaTitle`, `metaDescription`, `keywords`, `slug`) back to the article in the Halo CMS through a PATCH request. `metaDescription` and `keywords` are stored through `annotations`.

### LLM Metadata Generation

*   **Send Request to LLM**: The `SeoOptimizer.optimizeArticle` method constructs a detailed prompt, including the generation requirements for the SEO metadata (length, format, etc.), and sends it to the configured OpenAI-compatible API endpoint.
*   **Receive LLM Response**: Receives the JSON format response returned by the LLM.
*   **Parse LLM Output**: Parses the JSON string returned by the LLM into an `SeoMeta` object.
*   **Retry Mechanism**: Includes a built-in exponential backoff retry logic to increase the robustness of successful LLM calls.

### Article Data Processing

*   **Extract Article Information from Halo Raw Data**: The `HaloClient.extractArticleData` method parses the original Halo article object and extracts key information such as `article_id`, `title`, `content`, `excerpt`, `tags`, `categories`, `url`, and `slug`.
*   **Filter Articles**: During the extraction process, it automatically skips `deleted` or `unpublished` articles.
*   **Generate Content Hash**: Generates a SHA256 hash for the article content, which is used to determine whether the article content has changed, and thus whether it needs to be re-optimized.
*   **Save Articles to Database**: The `DatabaseManager.saveArticle` method saves or updates the extracted and processed article data to the `articles` table of the local SQLite database.

### SEO Metadata Validation

*   The `SeoValidator.validateSeoMeta` method performs multiple strict validations on the `SeoMeta` object generated by the LLM:
    *   `metaTitle`: Checks if it is empty and if the length exceeds 60 characters.
    *   `metaDescription`: Checks if it is empty and if the length is strictly between 80 and 160 characters.
    *   `keywords`: Checks if it is a non-empty array, if the length is between 2 and 5, and if each keyword is not empty.
    *   `slug`: Checks if it is empty and uses a regular expression to verify that it conforms to the format of short English words plus hyphens.

### Final Publishing

*   The `SeoPublisher.publishSeoMeta` method publishes the SEO metadata generated by the `SeoOptimizer` and validated by the `SeoValidator` to the Halo CMS through the `HaloClient`.

### Scheduler Operations

*   **Task Start**: The `Scheduler.start` method starts a cron scheduled task (defaults to once per hour) and a queue processor (once every 10 seconds), and immediately triggers an article enqueuing operation.
*   **Task Stop**: The `Scheduler.stop` method stops the cron task and the queue processor, and waits for any currently processing tasks to complete to ensure a graceful shutdown.
*   **Article Enqueuing**: The `Scheduler.enqueueArticlesForOptimization` method queries the database for articles that need to be optimized (based on the last optimization time, content hash changes, and forced re-optimization settings) and adds them to the internal task queue.
*   **Article Dequeuing and Processing**: The `Scheduler.processQueue` method takes an article from the task queue and executes the LLM optimization, SEO validation, and SEO publishing processes in sequence.
*   **Retry Mechanism**: The `SeoOptimizer` includes a retry mechanism for LLM calls.
*   **Status Recording**: After each article is processed, regardless of success or failure, the detailed running status, error messages, and LLM-related indicators are recorded through `DatabaseManager.recordSeoRun`.

### Database Operations

*   **Initialization**: The `DatabaseManager.initDatabase` method is responsible for creating the `seo_manager.db` file (if it does not exist) and initializing the `articles`, `seo_runs`, and `settings` tables, as well as creating the necessary indexes.
*   **Save Article**: `DatabaseManager.saveArticle` is used to insert or update article data into the `articles` table.
*   **Get Article**: `DatabaseManager.getAllArticles` and `DatabaseManager.getArticleById` are used to query article data.
*   **Record SEO Run Status**: `DatabaseManager.recordSeoRun` records the detailed results of each SEO optimization attempt, including the generated SEO metadata, status (success, failure, validation failure, etc.), error messages, and LLM usage indicators.
*   **Manage Settings**: `DatabaseManager` manages the application's runtime settings, such as `min_content_length`, `max_content_length`, `min_days_since_last_optimization`, and `force_reoptimize`, and provides the `getSetting` and `setSetting` methods.

### Logging Strategy

The project uses Winston as the logging library to implement a detailed and layered logging strategy:

*   **File Division of Labor**:
    *   `application.%DATE%.log`: Records general application-level logs (`info` level and above), without detailed debugging information for specific modules.
    *   `[moduleName].%DATE%.log`: Creates a separate log file for each core module (`haloClient`, `llmService`, `seoValidator`, `seoPublisher`, `scheduler`, `database`, etc.). These files default to recording all detailed information at the `debug` level and above for module-level troubleshooting.
    *   `llmSeoGenerations.%DATE%.log`: Specifically used to record the SEO metadata generated by the LLM, at the `info` level. It is written through the `logLLMSeoGeneration` function, and its content structure includes the `seoMeta` object, which is convenient for subsequent analysis of the LLM output quality.
    *   `exceptions.%DATE%.log`: Records uncaught exceptions.
    *   `rejections.%DATE%.log`: Records unhandled promise rejections.
*   **Naming**: Log files are in the format `[moduleName].YYYY-MM-DD.log` and support log file rotation and compression archiving by date.
*   **Content Structure**: Each log message includes a timestamp, log level, module name, and message content. For logs that contain additional context information (`meta`), this information is appended to the message as a JSON string for structured analysis.
*   **Level Application**:
    *   `fatal`, `error`, `warn`: Used to indicate serious errors, warnings, and potential problems.
    *   `info`: Used to record key events and status changes of the application.
    *   `debug`: Used to record detailed execution processes and variable states, mainly for development and debugging.
    *   `trace`: (New) Used to record the most fine-grained code execution paths, providing more in-depth debugging information.
*   **Console Output**: The console defaults to displaying only logs at the `info` level and above, and uses colors to distinguish between different levels for real-time monitoring of key information.

## 4. Future Prospects (Optional)

*   **Richer LLM Integration**: Explore support for more LLM providers, such as Google Gemini, Claude, etc., to provide more flexible choices.
*   **Configurable SEO Rules**: Allow users to customize SEO validation rules (e.g., the length range of `metaDescription`) through a configuration file or UI.
*   **Performance Monitoring and Reporting**: Add monitoring and reporting functions for performance indicators such as LLM call costs and response times.
*   **Webhook Notifications**: Send notifications to a specified webhook after SEO optimization is complete (success or failure).
*   **UI Interface**: Develop a simple web interface for viewing optimization status, managing settings, and manually triggering optimization.
